{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # PDF to Podcast Generator\n",
    "\n",
    "  This notebook converts PDF documents into engaging podcast-style audio content using AI. The goial was to create a tool that allows me understanding pdf by just listening for example while biking. This pdf can be fromn the internet as it can be my notes for a presentation, which is really fun to listen to compared to reading them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Import Required Libraries\n",
    "\n",
    "  First, we'll import all the libraries we need for processing PDFs, generating text, and creating audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "import edge_tts\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Load Environment Variables\n",
    "\n",
    "  We need to load the API key from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Set Up Constants\n",
    "\n",
    "  These are the main settings we'll use throughout the program I decided on grok-beta as XAI has a open testing for up to 25 euros of credits, which was plenty for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI_API_KEY = os.getenv(\"XAI_API_KEY\")\n",
    "XAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\n",
    "MODEL = \"grok-beta\"\n",
    "AVERAGE_SPEAKING_SPEED_WPM = 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## PDF Text Extraction\n",
    "\n",
    "  This function gets all the text from a PDF file and organizes it by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF with better error handling and metadata.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text_data = []\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            text_data.append({\n",
    "                'text': text,\n",
    "                'page': page_num + 1,\n",
    "                'chars': len(text)\n",
    "            })\n",
    "        return text_data\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Text Chunking\n",
    "\n",
    "  This function splits the text into chunks based on a maximum length of input to the summarizer (1024 for facebook/Bart-large-cnn) to keep the context of every page in summariessed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_page(text, max_tokens=1024):\n",
    "    \"\"\"Split text into equal size chunks if it exceeds the max token limit.\"\"\"\n",
    "    total_tokens = count_tokens(text)\n",
    "    if total_tokens <= max_tokens:\n",
    "        return [text]\n",
    "\n",
    "    num_chunks = (total_tokens + max_tokens - 1) // max_tokens\n",
    "    chunk_size = total_tokens // num_chunks\n",
    "\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, total_tokens, chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count the number of tokens in a text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Text Summarization\n",
    "\n",
    "  This function summarizes text using a Hugging Face pipeline. It chunks the text into smaller pieces to avoid exceeding the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize text using a Hugging Face pipeline.\"\"\"\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        chunks = chunk_page(text)\n",
    "        summaries = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
    "            summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "        return ' '.join(summaries)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error summarizing text: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Summarize each chunk of text\n",
    "def summarize_text_data(text_data):\n",
    "    summarized_text_data = []\n",
    "    for item in text_data:\n",
    "        summary = summarize_text(item['text'])\n",
    "        if summary:\n",
    "            summarized_text_data.append({\n",
    "                'text': summary,\n",
    "                'page': item['page'],\n",
    "                'chars': len(summary)\n",
    "            })\n",
    "    return summarized_text_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## File Processing\n",
    "\n",
    "  This function handles the uploaded PDF file safely for the extraction of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(uploaded_file):\n",
    "    \"\"\"Process uploaded file and create temporary file.\"\"\"\n",
    "    try:\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            tmp_file.write(uploaded_file.getvalue())\n",
    "            return tmp_file.name\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing uploaded file: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Summaries Grouping\n",
    "\n",
    "  This function groups the summaries text into chunks based on a maximum length (Listenting time) to keep the podcast in the desired length. Every grouping is passed then to the model to create a dialogue between the host and the expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_grouper(summarized_text_data, podcast_length):\n",
    "    \"\"\"Split summarized text data into chunks based on user-defined podcast length.\"\"\"\n",
    "    if podcast_length not in [5, 10, 15]:\n",
    "        st.error(\"Invalid podcast length. Choose 5, 10, or 15.\")\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    num_summaries = len(summarized_text_data[1:])\n",
    "    summaries_per_chunk = num_summaries // podcast_length\n",
    "\n",
    "    for i in range(0, num_summaries, summaries_per_chunk):\n",
    "        chunk = summarized_text_data[1:][i:i + summaries_per_chunk]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Script Generation\n",
    "\n",
    "  This function turns our text into a natural conversation between a host and expert. I use the same formatting for both but change the label, this way its easy to process the audio. It First creates a introduction using the data from the first page, then creates a Q&A section for each chunk of the text, and finally creates a conclusion based on the script. This way it creates a natural flow for the podcast and keeps the name of the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_podcast_script(summarized_text_data, first_page_text, podcast_length):\n",
    "    try:\n",
    "        dialogues = []\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {XAI_API_KEY}'\n",
    "        }\n",
    "\n",
    "        # System message for the AI model passed on every prompt\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an expert podcast script writer. Your task is to:\n",
    "                    1. Convert technical content into engaging dialogue\n",
    "                    2. Maintain accuracy while making content accessible\n",
    "                    3. Create natural transitions between topics\n",
    "                    4. Include relevant examples and analogies\n",
    "                    5. Keep a consistent tone throughout the conversation\n",
    "                    REQUIREMENTS:\n",
    "                    1. Use 'Host' and 'Expert' as speakers\n",
    "                    2. Always Format as (Never change the words **Host**: and **:Expert**: in formatting only inside dialogue):\n",
    "                       **Host:** [dialogue]\n",
    "                       **Expert:** [dialogue]\"\"\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            'model': MODEL,\n",
    "            'temperature': 0.7,\n",
    "            'max_tokens': 1000,\n",
    "            'top_p': 0.9,\n",
    "            'frequency_penalty': 0.3,\n",
    "            'presence_penalty': 0.2\n",
    "        }\n",
    "\n",
    "        # Generate introduction\n",
    "        introduction_prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Create an engaging introduction for the podcast that includes:\n",
    "                1. A brief overview of the content\n",
    "                2. An introduction of the host 'Diego' as Host \n",
    "                3. An introduction of an author, if no writters name refer in the dialog as expert\n",
    "                3. A hook to capture the audience's attention\n",
    "                4. Mention that this is an AI-generated podcast\n",
    "                CONTENT:\n",
    "                {first_page_text}\n",
    "                TEMPLATE:\n",
    "                Host: Welcome to \"Diego's AI podcasts,\" your go-to podcast for diving into complex topics with the help of artificial intelligence! I'm your host, Diego, and today we have a special guest who is an expert in [Expertise Area]. Joining us is [Expert Name], a [brief description of expertise]. They've been at the forefront of [mention relevant achievements or contributions]. Today, we'll be exploring [brief overview of podcast topic], delving deep into [specific aspect of the topic]. Get ready to dive into the fascinating world of [Theme/Topic]!\n",
    "                Expert: Thank you, Diego! I'm thrilled to be here and discuss [specific aspect of the topic]. It's an exciting time in [Expertise Area], and I'm looking forward to sharing insights and exploring new ideas with you.  Let's get started!\"\"\"\n",
    "        }\n",
    "\n",
    "        data['messages'] = [system_message, introduction_prompt]\n",
    "        response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            introduction = response.json()['choices'][0]['message']['content']\n",
    "            dialogues.append(introduction)\n",
    "        else:\n",
    "            st.error(f\"API error for introduction: {response.status_code} - {response.text}\")\n",
    "\n",
    "        # Split summarized text data into chunks based on desired podcast length\n",
    "\n",
    "        # Map podcast length to number of chunks\n",
    "        length_mapping = {\n",
    "            \"Short (15 min)\": 5,\n",
    "            \"Medium (30 min)\": 10,\n",
    "            \"Long (45 min)\": 15\n",
    "        }\n",
    "\n",
    "        # Get the number of chunks based on the selected podcast length\n",
    "        num_chunks = length_mapping.get(podcast_length)\n",
    "        chunks = summary_grouper(summarized_text_data, num_chunks)\n",
    "\n",
    "        # Generate Q&A section for each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            combined_text = ' '.join([item['text'] for item in chunk])\n",
    "            qna_prompt = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Create a Q&A section based on the following content:\n",
    "                CONTENT:\n",
    "                {combined_text}\n",
    "                1. Include:\n",
    "                   - 1-2 clarifying questions from the Host\n",
    "                   - Real-world examples or analogies\n",
    "                   - Natural transitions\n",
    "                   - Relevant technical details\n",
    "                   - Paraphrase the questions and answers in a podcast-friendly way, not a literal Q&A\n",
    "                   - Maintain technical accuracy while being conversational\n",
    "                   - Keep responses concise (2-3 sentences per speaker turn)\n",
    "                   - Do not use phrases like today we are talking about, the topic of today is, etc.\n",
    "                2. REQUIREMENTS:\n",
    "                    1. Use 'Host' and 'Expert' as speakers\n",
    "                    2. Always Format as (Never change the words **Host**: and **:Expert**: in formatting only inside dialogue):\n",
    "                       **Host:** [dialogue]\n",
    "                       **Expert:** [dialogue]\"\"\"\n",
    "            }\n",
    "\n",
    "            data['messages'] = [system_message, qna_prompt]\n",
    "            response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "            if response.status_code == 200:\n",
    "                qna = response.json()['choices'][0]['message']['content']\n",
    "                dialogues.append(qna)\n",
    "            else:\n",
    "                st.error(f\"API error for Q&A section {i+1}: {response.status_code} - {response.text}\")\n",
    "\n",
    "        # Generate conclusion\n",
    "        conclusion_prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Create a conclusion that:\n",
    "                1. Summarizes most relevant points from the following Q&A sections:\n",
    "                {''.join(dialogues[1:])} while still keeping in the context of the cover page (name of Expert, pdf objective): {dialogues[0]}\n",
    "                2. Provides a memorable takeaway\n",
    "                3. Includes a call to action\n",
    "                4. Thanks the audience\n",
    "                5. In the dialog always refer to the host by their name and the expert as expert\n",
    "                Keep it under 1 minute when spoken.\n",
    "                REQUIREMENTS:\n",
    "                    1. Use 'Host' and 'Expert' as speakers\n",
    "                    2. Refer in Dialog to host as Diego and expert as the name of the expert\n",
    "                    3. Always Format as (Never change the words **Host**: and **:Expert**: in formatting only inside dialogue):\n",
    "                       **Host:** [dialogue]\n",
    "                       **Expert:** [dialogue]\"\"\"\n",
    "        }\n",
    "\n",
    "        data['messages'] = [system_message, conclusion_prompt]\n",
    "        response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            conclusion = response.json()['choices'][0]['message']['content']\n",
    "            dialogues.append(conclusion)\n",
    "        else:\n",
    "            st.error(f\"API error for conclusion: {response.status_code} - {response.text}\")\n",
    "\n",
    "        return '\\n\\n'.join(dialogues)\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error generating podcast script: {str(e)}\")\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Audio Processing\n",
    "\n",
    "  These functions handle creating and combining audio files. This way it can create 2 of them then concatenate them and create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_audio(file_list, output_path):\n",
    "    \"\"\"Concatenate audio files using FFmpeg\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "        for audio_file in file_list:\n",
    "            f.write(f\"file '{audio_file}'\\n\")\n",
    "        concat_list = f.name\n",
    "\n",
    "    try:\n",
    "        command = [\n",
    "            'ffmpeg', '-f', 'concat', '-safe', '0',\n",
    "            '-i', concat_list,\n",
    "            '-c:a', 'pcm_s16le',\n",
    "            '-ar', '16000',\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, capture_output=True, check=True)\n",
    "    finally:\n",
    "        os.unlink(concat_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Text-to-Speech Generation\n",
    "\n",
    "  This function converts our script into spoken audio using different voices. I tried gtts but it doesn't allow for 2 voices, and some of the more complex ones require more setup. Edge_tts was the best one I could find with multiple voices, this still sounds a bit robotic but still can be listened to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_audio(dialogue_text):\n",
    "    try:\n",
    "        st.write(\"Starting generate_audio function\")\n",
    "        st.write(f\"Input dialogue_text length: {len(dialogue_text)}\")\n",
    "\n",
    "        # Create a temporary directory to store audio files\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            audio_files = []\n",
    "            lines = dialogue_text.splitlines()\n",
    "            st.write(f\"Number of lines split: {len(lines)}\")\n",
    "\n",
    "            for i, line in enumerate(lines):\n",
    "                st.write(f\"Processing line {i}: {line}\")\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "\n",
    "                if line.startswith(\"**Host:**\") or line.startswith(\"**Expert:**\"):\n",
    "                    speaker, _, text = line.partition(':')\n",
    "                    text = text.strip().lstrip(\"*\").strip()\n",
    "                    speaker = speaker.strip().lstrip(\"*\").strip()\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    # Choose voice based on speaker\n",
    "                    if speaker == \"Host\":\n",
    "                        voice = \"en-US-GuyNeural\"    # Male voice for Host\n",
    "                    elif speaker == \"Expert\":\n",
    "                        voice = \"en-US-JennyNeural\"  # Female voice for Expert\n",
    "                    else:\n",
    "                        st.error(f\"Unknown speaker '{speaker}' in line {i}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        temp_path = os.path.join(temp_dir, f'segment_{i}.mp3')\n",
    "\n",
    "                        # Generate speech asynchronously\n",
    "                        communicate = edge_tts.Communicate(text, voice)\n",
    "                        asyncio.run(communicate.save(temp_path))\n",
    "\n",
    "                        audio_files.append(temp_path)\n",
    "\n",
    "                    except Exception as inner_e:\n",
    "                        st.error(f\"Error generating audio for line {i}: {str(inner_e)}\")\n",
    "                        continue\n",
    "\n",
    "            if audio_files:\n",
    "                # Convert mp3 files to wav format for concatenation\n",
    "                wav_files = []\n",
    "                for mp3_file in audio_files:\n",
    "                    wav_file = mp3_file.replace('.mp3', '.wav')\n",
    "                    command = [\n",
    "                        'ffmpeg', '-y', '-i', mp3_file,\n",
    "                        '-ar', '16000',\n",
    "                        '-ac', '1',\n",
    "                        '-c:a', 'pcm_s16le',\n",
    "                        wav_file\n",
    "                    ]\n",
    "                    subprocess.run(command, capture_output=True, check=True)\n",
    "                    wav_files.append(wav_file)\n",
    "\n",
    "                output_path = os.path.join(temp_dir, 'combined.wav')\n",
    "                concatenate_audio(wav_files, output_path)\n",
    "\n",
    "                # Read the final audio file\n",
    "                with open(output_path, 'rb') as f:\n",
    "                    return f.read()\n",
    "\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in generate_audio: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Streamlit Web Interface\n",
    "\n",
    "  This creates our user interface for uploading PDFs and generating podcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"PDF to Podcast Generator\")\n",
    "    st.write(\"Upload a PDF to generate a dialogue script and convert it to audio with distinct voices.\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload your PDF\", type=[\"pdf\"], accept_multiple_files=False)\n",
    "\n",
    "    # Add a selectbox for podcast length\n",
    "    podcast_length = st.selectbox(\"Select Podcast Length\", [\"Short (15 min)\", \"Medium (30 min)\", \"Long (45 min)\"])\n",
    "\n",
    "    generate_button = st.button(\"Generate Script and Audio\")\n",
    "\n",
    "    if generate_button and uploaded_file is not None:\n",
    "        try:\n",
    "            progress_bar = st.progress(0)\n",
    "            status_text = st.empty()\n",
    "\n",
    "            with st.spinner(\"Creating podcast, Please wait this might take a while...\"):\n",
    "                progress_bar.progress(10)\n",
    "                status_text.text(\"Step 1: Processing uploaded file...\")\n",
    "                temp_pdf_path = process_uploaded_file(uploaded_file)\n",
    "                st.write(f\"Processed PDF Successfully\")\n",
    "\n",
    "                if temp_pdf_path:\n",
    "                    progress_bar.progress(30)\n",
    "                    status_text.text(\"Step 2: Extracting text from PDF...\")\n",
    "                    text_data = extract_text_from_pdf(temp_pdf_path)\n",
    "                    if not text_data:\n",
    "                        st.error(\"Failed to extract text from the PDF.\")\n",
    "                        return\n",
    "                    st.write(f\"Extracted text from PDF Successfully\")\n",
    "\n",
    "                    progress_bar.progress(50)\n",
    "                    status_text.text(\"Step 3: Summarizing text data...\")\n",
    "                    summarized_text_data = summarize_text_data(text_data)\n",
    "                    if not summarized_text_data:\n",
    "                        st.error(\"Failed to summarize text from the PDF. Pages may be too long.\")\n",
    "                        return\n",
    "                    st.write(f\"Summarized text data Successfully\")\n",
    "\n",
    "                    progress_bar.progress(70)\n",
    "                    status_text.text(\"Step 4: Generating podcast script...\")\n",
    "                    first_page_text = text_data[0]['text'] + ' '.join([item['text'] for item in summarized_text_data[0:3]]) if text_data else ' '.join([item['text'] for item in summarized_text_data[0:3]])\n",
    "                    full_script = generate_podcast_script(summarized_text_data, first_page_text, podcast_length)\n",
    "                    if full_script:\n",
    "                        st.subheader(\"Generated Script\")\n",
    "                        st.text_area(\"Script\", full_script, height=400)\n",
    "\n",
    "                        progress_bar.progress(90)\n",
    "                        status_text.text(\"Step 5: Generating audio...\")\n",
    "                        audio_data = generate_audio(full_script)\n",
    "                        if audio_data:\n",
    "                            st.session_state['audio_data'] = audio_data\n",
    "                            st.audio(audio_data, format=\"audio/wav\")\n",
    "\n",
    "                            st.download_button(\n",
    "                                \"Download Complete Podcast Audio\",\n",
    "                                st.session_state['audio_data'],\n",
    "                                file_name=\"complete_podcast.wav\",\n",
    "                                mime=\"audio/wav\",\n",
    "                                key=\"download_button\"\n",
    "                            )\n",
    "                        else:\n",
    "                            st.error(\"Failed to generate audio.\")\n",
    "                    else:\n",
    "                        st.error(\"Failed to generate script.\")\n",
    "\n",
    "                    os.unlink(temp_pdf_path)\n",
    "\n",
    "                progress_bar.progress(100)\n",
    "                status_text.text(\"Process completed successfully!\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            st.warning(\"Process interrupted by user.\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            st.stop()\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This file is originally a .py so I can run it with streamlit, but i converted it into a notebook to add explanations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
