{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ea3cc1-79ea-4910-970a-43d28e869bcd",
   "metadata": {},
   "source": [
    " # PDF to Podcast Generator\n",
    " This notebook converts PDF documents into engaging podcast-style audio content using AI. The goial was to create a tool that allows me understanding pdf by just listening for example while biking. This pdf can be fromn the internet as it can be my notes for a presentation, which is really fun to listen to compared to reading them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d93ce-39b6-4e51-8f91-b3e2fad493da",
   "metadata": {},
   "source": [
    " ## Import Required Libraries\n",
    " First, we'll import all the libraries we need for processing PDFs, generating text, and creating audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57938644-b19c-4256-aa1c-56f54288a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23098de5-85f4-4596-b5cc-6faefaae58ae",
   "metadata": {},
   "source": [
    " ## Load Environment Variables\n",
    " We need to load the API key from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a14ce93-a8ef-47cf-ac0b-69f43916f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627e006-ad39-4d50-b11b-a05a9a66a972",
   "metadata": {},
   "source": [
    " ## Set Up Constants\n",
    " These are the main settings we'll use throughout the program I decided on grok-beta as XAI has a open testing for up to 25 euros of credits, which was plenty for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f6c6c-5f9f-43c5-96c5-d0ca22ed5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI_API_KEY = os.getenv(\"XAI_API_KEY\")\n",
    "XAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\n",
    "MODEL = \"grok-beta\"\n",
    "AVERAGE_SPEAKING_SPEED_WPM = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763198e-1c61-482e-956f-b6d2544c9a81",
   "metadata": {},
   "source": [
    " ## PDF Text Extraction\n",
    " This function gets all the text from a PDF file and organizes it by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d989e5b-ceed-4048-a973-1e538717cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF with better error handling and metadata.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text_data = []\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            text_data.append({\n",
    "                'text': text,\n",
    "                'page': page_num + 1,\n",
    "                'chars': len(text)\n",
    "            })\n",
    "        return text_data\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Summarize text using a Hugging Face pipeline.\"\"\"\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error summarizing text: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Summarize each chunk of text\n",
    "def summarize_text_data(text_data):\n",
    "    summarized_text_data = []\n",
    "    for item in text_data:\n",
    "        summary = summarize_text(item['text'])\n",
    "        if summary:\n",
    "            summarized_text_data.append({\n",
    "                'text': summary,\n",
    "                'page': item['page'],\n",
    "                'chars': len(summary)\n",
    "            })\n",
    "    return summarized_text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f4610-a109-4d21-b466-48f34b472e99",
   "metadata": {},
   "source": [
    " ## Text Chunking\n",
    " We break down the text into smaller, manageable pieces for grok while keeping sentences together. I use overlapping words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9533a7-ac85-49fd-9530-310dc514df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for page_data in text_data:\n",
    "        text = page_data['text']\n",
    "        words = text.split()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "\n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            current_size += len(word) + 1  # +1 for space\n",
    "\n",
    "            # Check if chunk is complete at end of sentence\n",
    "            if current_size >= chunk_size and word[-1] in '.!?':\n",
    "                chunks.append({\n",
    "                    'text': ' '.join(current_chunk),\n",
    "                    'page': page_data['page'],\n",
    "                    'size': current_size\n",
    "                })\n",
    "                # Keep overlap words for context\n",
    "                overlap_words = current_chunk[-int(overlap/5):]  # Approximate words for overlap\n",
    "                current_chunk = overlap_words\n",
    "                current_size = sum(len(word) + 1 for word in overlap_words)\n",
    "\n",
    "        # Add remaining chunk if it's substantial\n",
    "        if current_size > overlap:\n",
    "            chunks.append({\n",
    "                'text': ' '.join(current_chunk),\n",
    "                'page': page_data['page'],\n",
    "                'size': current_size\n",
    "            })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c07d25-421f-4dd2-85c9-aa51eb6d0f24",
   "metadata": {},
   "source": [
    " ## File Processing\n",
    " This function handles the uploaded PDF file safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a4f04c-b082-466f-8155-d9985180a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(uploaded_file):\n",
    "    \"\"\"Process uploaded file and create temporary file.\"\"\"\n",
    "    try:\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            tmp_file.write(uploaded_file.getvalue())\n",
    "            return tmp_file.name\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing uploaded file: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e6864-5402-45d6-b02b-72ce414a6cff",
   "metadata": {},
   "source": [
    " ## Word Count Management\n",
    " These functions help control the length of the final podcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5ee14f-a4b8-49bb-80ac-10bd9ab7499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_words(max_minutes):\n",
    "    return max_minutes * AVERAGE_SPEAKING_SPEED_WPM\n",
    "\n",
    "def trim_chunks_to_max_words(chunks, max_words):\n",
    "    try:\n",
    "        total_words = 0\n",
    "        trimmed_chunks = []\n",
    "        for chunk in chunks:\n",
    "            chunk_word_count = len(chunk.split())\n",
    "            if total_words + chunk_word_count > max_words and trimmed_chunks:\n",
    "                break\n",
    "            trimmed_chunks.append(chunk)\n",
    "            total_words += chunk_word_count\n",
    "        if not trimmed_chunks and chunks:\n",
    "            trimmed_chunks.append(chunks[0])  # Ensure at least one chunk\n",
    "        st.write(f\"Trimmed to {len(trimmed_chunks)} chunks\")  # Debugging info\n",
    "        return trimmed_chunks\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error trimming chunks: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22c1ad-b97a-4a8d-8289-390b6cf0e3e1",
   "metadata": {},
   "source": [
    " ## Script Generation\n",
    " This function turns our text into a natural conversation between a host and expert. I use the same formatting for both but change the label, this way its easy to process the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8890f6f-9be9-4717-8068-8ad7db4bc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_podcast_script(chunks, format_type):\n",
    "    try:\n",
    "        dialogues = []\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': f'Bearer {XAI_API_KEY}'\n",
    "        }\n",
    "\n",
    "        # Initial system message to set context\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an expert podcast script writer. Your task is to:\n",
    "                            1. Convert technical content into engaging dialogue\n",
    "                            2. Maintain accuracy while making content accessible\n",
    "                            3. Create natural transitions between topics\n",
    "                            4. Include relevant examples and analogies\n",
    "                            5. Keep a consistent tone throughout the conversation\"\"\"\n",
    "        }\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Extract metadata if available\n",
    "\n",
    "            if format_type == \"Podcast\":\n",
    "                messages = [\n",
    "                    system_message,\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"\n",
    "                        Convert this content into a natural podcast dialogue:\n",
    "\n",
    "                        CONTENT:\n",
    "                        {chunk}\n",
    "\n",
    "                        REQUIREMENTS:\n",
    "                        1. Use 'Host' and 'Expert' as speakers\n",
    "                        2. Format as:\n",
    "                           **Host:** [dialogue]\n",
    "                           **Expert:** [dialogue]\n",
    "                        3. Include:\n",
    "                           - 1-2 clarifying questions from the Host\n",
    "                           - Real-world examples or analogies\n",
    "                           - Natural transitions\n",
    "                        4. Maintain technical accuracy while being conversational\n",
    "                        5. Keep responses concise (2-3 sentences per speaker turn)\n",
    "                        \"\"\"\n",
    "                    }\n",
    "                ]\n",
    "            else: return \"Invalid format type\"\n",
    "\n",
    "            data = {\n",
    "                'model': MODEL,\n",
    "                'messages': messages,\n",
    "                'temperature': 0.7,\n",
    "                'max_tokens': 1000,\n",
    "                'top_p': 0.9,\n",
    "                'frequency_penalty': 0.2,\n",
    "                'presence_penalty': 0.2\n",
    "            }\n",
    "\n",
    "            st.write(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "            response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                dialogue = result['choices'][0]['message']['content']\n",
    "                dialogues.append(dialogue)\n",
    "\n",
    "                # Add transition prompt if not the last chunk\n",
    "                if i < len(chunks) - 1:\n",
    "                    transition_prompt = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Generate a smooth transition to the next topic that maintains flow and engagement.\"\n",
    "                    }\n",
    "                    data['messages'] = [system_message, transition_prompt]\n",
    "                    transition_response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "                    if transition_response.status_code == 200:\n",
    "                        transition = transition_response.json()['choices'][0]['message']['content']\n",
    "                        dialogues.append(transition)\n",
    "            else:\n",
    "                st.error(f\"API error for chunk {i+1}: {response.status_code} - {response.text}\")\n",
    "\n",
    "        # Generate conclusion This section still could be fixed \n",
    "        conclusion_prompt = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Create a conclusion that:\n",
    "                1. Summarizes key points\n",
    "                2. Provides a memorable takeaway\n",
    "                3. Includes a call to action\n",
    "                4. Thanks the audience\n",
    "                Keep it under 1 minute when spoken.\"\"\"\n",
    "        }\n",
    "\n",
    "        data['messages'] = [system_message, conclusion_prompt]\n",
    "        conclusion_response = requests.post(XAI_API_URL, headers=headers, json=data)\n",
    "        if conclusion_response.status_code == 200:\n",
    "            conclusion = conclusion_response.json()['choices'][0]['message']['content']\n",
    "            dialogues.append(conclusion)\n",
    "\n",
    "        return '\\n\\n'.join(dialogues)\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error generating podcast script: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d4ecd-0031-46c2-96e6-7df23793c3a0",
   "metadata": {},
   "source": [
    " ## Audio Processing\n",
    " These functions handle creating and combining audio files. This way it can create 2 of them then concatenate them and create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b05ef01-a8e6-4fb6-af60-c753c8cdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_audio(file_list, output_path):\n",
    "    \"\"\"Concatenate audio files using FFmpeg\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "        for audio_file in file_list:\n",
    "            f.write(f\"file '{audio_file}'\\n\")\n",
    "        concat_list = f.name\n",
    "\n",
    "    try:\n",
    "        command = [\n",
    "            'ffmpeg', '-f', 'concat', '-safe', '0',\n",
    "            '-i', concat_list,\n",
    "            '-c:a', 'pcm_s16le',\n",
    "            '-ar', '16000',\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, capture_output=True, check=True)\n",
    "    finally:\n",
    "        os.unlink(concat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8596acf-6435-4348-93ba-366515627222",
   "metadata": {},
   "source": [
    " ## Text-to-Speech Generation\n",
    " This function converts our script into spoken audio using different voices. I tried gtts but it doesn't allow for 2 voices, and some of the more complex ones require more setup. Edge_tts was the best one I could find with multiple voices, this still sounds a bit robotic but still can be listened to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "380b67ee-4dad-4bf2-aeb5-875fc0ebecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import edge_tts\n",
    "\n",
    "def generate_audio(dialogue_text):\n",
    "    try:\n",
    "        st.write(\"Starting generate_audio function\")\n",
    "        st.write(f\"Input dialogue_text length: {len(dialogue_text)}\")\n",
    "\n",
    "        # Create a temporary directory to store audio files\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            audio_files = []\n",
    "            lines = dialogue_text.splitlines()\n",
    "            st.write(f\"Number of lines split: {len(lines)}\")\n",
    "\n",
    "            for i, line in enumerate(lines):\n",
    "                st.write(f\"Processing line {i}: {line}\")\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "\n",
    "                if line.startswith(\"**Host:**\") or line.startswith(\"**Expert:**\"):\n",
    "                    speaker, _, text = line.partition(':')\n",
    "                    st.write(f\"Raw speaker: {speaker}\")\n",
    "                    text = text.strip().lstrip(\"*\").strip()\n",
    "                    speaker = speaker.strip().lstrip(\"*\").strip()\n",
    "                    st.write(f\"Processed speaker: {speaker}\")\n",
    "                    st.write(f\"Text: {text}\")\n",
    "\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    # Choose voice based on speaker\n",
    "                    if speaker == \"Host\":\n",
    "                        voice = \"en-US-GuyNeural\"    # Male voice for Host\n",
    "                    elif speaker == \"Expert\":\n",
    "                        voice = \"en-US-JennyNeural\"  # Female voice for Expert\n",
    "                    else:\n",
    "                        st.error(f\"Unknown speaker '{speaker}' in line {i}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        temp_path = os.path.join(temp_dir, f'segment_{i}.mp3')\n",
    "\n",
    "                        # Generate speech asynchronously\n",
    "                        communicate = edge_tts.Communicate(text, voice)\n",
    "                        asyncio.run(communicate.save(temp_path))\n",
    "\n",
    "                        audio_files.append(temp_path)\n",
    "\n",
    "                    except Exception as inner_e:\n",
    "                        st.error(f\"Error generating audio for line {i}: {str(inner_e)}\")\n",
    "                        continue\n",
    "\n",
    "            if audio_files:\n",
    "                # Convert mp3 files to wav format for concatenation\n",
    "                wav_files = []\n",
    "                for mp3_file in audio_files:\n",
    "                    wav_file = mp3_file.replace('.mp3', '.wav')\n",
    "                    command = [\n",
    "                        'ffmpeg', '-y', '-i', mp3_file,\n",
    "                        '-ar', '16000',\n",
    "                        '-ac', '1',\n",
    "                        '-c:a', 'pcm_s16le',\n",
    "                        wav_file\n",
    "                    ]\n",
    "                    subprocess.run(command, capture_output=True, check=True)\n",
    "                    wav_files.append(wav_file)\n",
    "\n",
    "                output_path = os.path.join(temp_dir, 'combined.wav')\n",
    "                concatenate_audio(wav_files, output_path)\n",
    "\n",
    "                # Read the final audio file\n",
    "                with open(output_path, 'rb') as f:\n",
    "                    return f.read()\n",
    "\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in generate_audio: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3309bf-2103-4e2b-8553-dea20a6f6684",
   "metadata": {},
   "source": [
    " ## Streamlit Web Interface\n",
    " This creates our user interface for uploading PDFs and generating podcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299a7256-752a-4d83-9311-2f554e849912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"PDF to Podcast Generator\")\n",
    "    st.write(\"Upload a PDF to generate a dialogue script and convert it to audio with distinct voices.\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload your PDF\", type=\"pdf\", accept_multiple_files=False)\n",
    "\n",
    "    # Dropdown for content format\n",
    "    content_format = st.selectbox(\"Select Content Format\", [\"Podcast\", \"Short-form Video Script (TikTok, Reels, Shorts)\"])\n",
    "\n",
    "    # Input for max podcast length\n",
    "    max_length = st.number_input(\"Maximum Length (minutes)\", min_value=1, max_value=60, value=5, step=1)\n",
    "\n",
    "    # Button to generate script and audio\n",
    "    generate_button = st.button(\"Generate Script and Audio\")\n",
    "\n",
    "    if generate_button and uploaded_file is not None:\n",
    "        try:\n",
    "            with st.spinner(\"Processing PDF and creating script...\"):\n",
    "                # Save uploaded file temporarily\n",
    "                temp_pdf_path = process_uploaded_file(uploaded_file)\n",
    "\n",
    "                if temp_pdf_path:\n",
    "                    # Extract text from PDF\n",
    "                    text_data = extract_text_from_pdf(temp_pdf_path)\n",
    "                    if not text_data:\n",
    "                        st.error(\"Failed to extract text from the PDF.\")\n",
    "                        return\n",
    "\n",
    "                    # Create text chunks\n",
    "                    chunks_data = chunk_text(text_data)\n",
    "                    chunks = [chunk['text'] for chunk in chunks_data]\n",
    "\n",
    "                    if chunks:\n",
    "                        st.success(\"Successfully created text chunks!\")\n",
    "\n",
    "                        # Trim chunks to fit desired podcast length\n",
    "                        max_words = calculate_max_words(max_length)\n",
    "                        trimmed_chunks = trim_chunks_to_max_words(chunks, max_words)\n",
    "\n",
    "                        # Generate the script\n",
    "                        script = generate_podcast_script(trimmed_chunks, content_format)\n",
    "\n",
    "                        if script:\n",
    "                            st.subheader(\"Generated Script\")\n",
    "                            st.text_area(\"Script\", script, height=400)\n",
    "\n",
    "                            st.write(\"Generating audio...\")\n",
    "                            audio_data = generate_audio(script)\n",
    "                            if audio_data:\n",
    "                                # Play audio\n",
    "                                st.audio(audio_data, format=\"audio/wav\")\n",
    "\n",
    "                                # Download button with key to prevent app reset\n",
    "                                st.download_button(\n",
    "                                    \"Download Complete Podcast Audio\",\n",
    "                                    audio_data,\n",
    "                                    file_name=\"complete_podcast.wav\",\n",
    "                                    mime=\"audio/wav\",\n",
    "                                    key=\"download_button\"\n",
    "                                )\n",
    "                            else:\n",
    "                                st.error(\"Failed to generate audio.\")\n",
    "                        else:\n",
    "                            st.error(\"Failed to generate script.\")\n",
    "                    else:\n",
    "                        st.error(\"No text chunks were created from the PDF.\")\n",
    "\n",
    "                    # Cleanup temporary file\n",
    "                    os.unlink(temp_pdf_path)\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is originally a .py so I can run it with streamlit, but i converted it into a notebook to add explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfToPodcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
